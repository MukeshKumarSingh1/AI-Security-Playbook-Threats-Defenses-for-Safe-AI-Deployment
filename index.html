<?xml version="1.0" encoding="UTF-8"?><?xml-stylesheet href="treestyles.css" type="text/css"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><!--This file has been created with toxhtml.xsl--><head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type" /><title>
AI Security Playbook: Fundamentals, Threats & Defenses for Safe AI Deployment
    </title><link rel="stylesheet" href="Curated%20Guide%20to%20AI%20Security.html_files//treestyles.css" type="text/css" /><script type="text/javascript" src="Curated%20Guide%20to%20AI%20Security.html_files//marktree.js"> 
	</script></head><body><div class="basetop"><a onclick="expandAll(document.getElementById('base'))" href="#">Expand</a> -
<a onclick="collapseAll(document.getElementById('base'))" href="#">Collapse</a></div><div class="basetext" id="base"><ul>
	<li class="col" id="FMID_708975492FM"><div class="nodecontent" style="color:#000000;font-size:167%;"><p>
      AI Security Playbook: Fundamentals, Threats & Defenses for Safe AI Deployment
    </p></div>
		<ul class="subexp">
	<li class="exp" id="FMID_1482812998FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Core Principles of AI Security</div>
		<ul class="sub">
	<li class="col" id="FMID_856997758FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Introduction: What Is AI Security?</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1735809379FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><strong><font color="#000000">AI Security</font></strong><font color="#000000"> involves protecting artificial intelligence systems—covering models, data, software, and infrastructure—from threats and failures throughout their lifecycle. It ensures that AI behaves as intended, resists manipulation, protects sensitive data, supports accountability, and adheres to ethical standards. </font></p><p class="western" align="left" /><p class="western" align="left"><font color="#000000">The need for AI security has grown as AI is now embedded in healthcare, finance, critical infrastructure, and everyday technology. Attacks against AI can pose both technical risks (such as model theft or data leakage) and societal risks (such as discrimination or misuse).</font></p></div></li></ul></li>
	<li class="col" id="FMID_1337027450FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Confidentiality, Integrity, and Availability (CIA) for AI</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1823898416FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">The <strong>CIA Triad</strong>—a fundamental model in cybersecurity—also serves as the bedrock for AI security:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Confidentiality</font></strong><font color="#000000">: Ensures that sensitive data (training data, model parameters, outputs) is only accessible to authorized users. Unauthorized access or data leakage could expose personal information, proprietary models, or trade secrets.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Integrity</font></strong><font color="#000000">: Ensures data and AI models are not tampered with. Adversaries may try to manipulate training data (data poisoning), alter models, or inject false inputs, compromising the system’s reliability and trustworthiness.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Availability</font></strong><font color="#000000">: Guarantees that AI systems and services are reliable and accessible to authorized users when needed. Denial of service, resource exhaustion, or infrastructure attacks threaten AI system availability.</font></p></li></ul><p class="western" align="left"><font color="#000000">AI security goes beyond traditional contexts, requiring custom protections for the unique assets and characteristics of machine learning and autonomous systems.</font></p></div></li></ul></li>
	<li class="col" id="FMID_463448856FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Validity and Reliability</div>
		<ul class="subexp">
	<li class="basic" id="FMID_766077853FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Validity</font></strong><font color="#000000"> in AI security means that an AI model makes correct, fair, and expected decisions in line with its purpose—even in the face of attacks or novel data. This involves robust algorithm design, validation against adversarial scenarios, and continuous verification.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Reliability</font></strong><font color="#000000"> indicates the system’s consistent performance over time. Secure AI should resist manipulation attempts and continue delivering expected outcomes despite changing environments or inputs.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_225777428FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Safety Robustness and Resilience</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1536596920FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Safety</font></strong><font color="#000000"> in AI refers to minimizing harm—ensuring that an AI’s actions do not cause unintended damage or risk to users, society, or the environment.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Robustness</font></strong><font color="#000000"> is the system’s ability to handle errors, attacks, or unexpected input gracefully. A robust AI system is less likely to be fooled by adversarial attacks, out-of-distribution data, or benign errors.</font></p></li></ul><p class="western" align="left"><font color="#000000">Together, these properties support , which is a key aim of security engineering in AI.</font></p></div></li></ul></li>
	<li class="col" id="FMID_330267775FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Fairness and Non-Discrimination</div>
		<ul class="subexp">
	<li class="basic" id="FMID_511960826FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Fairness</font></strong><font color="#000000"> requires that AI systems do not systematically disadvantage individuals or groups based on race, gender, age, or other protected characteristics.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Non-Discrimination</font></strong><font color="#000000"> involves monitoring for and mitigating algorithmic bias. Data selection, model training, and output evaluation must be carefully managed to avoid reinforcing or introducing unfair outcomes.</font></p></li></ul><p class="western" align="left"><font color="#000000">Security professionals play a role here: attacks may exploit fairness weaknesses or induce bias, and responsible AI security includes defending against these threats.</font></p></div></li></ul></li>
	<li class="col" id="FMID_1391279462FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Transparency, Accountability, and Explainability</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1175832047FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Transparency</font></strong><font color="#000000"> means making clear how AI decisions are made, which data influences results, and what logic or rules are used. Transparent systems are easier to audit, trust, and secure.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Accountability</font></strong><font color="#000000"> ensures that actors (people or organizations) can be held responsible for the actions and outcomes of AI systems, especially when things go wrong.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Explainability</font></strong><font color="#000000"> is the ability to provide understandable, human-interpretable explanations for AI-driven outcomes or decisions. Explainable AI is essential for debugging, compliance, and building trust<u><a href="https://www.paloaltonetworks.com/cyberpedia/ai-security" target="_blank">8</a></u>.</font></p></li></ul><p class="western" align="left"><font color="#000000">Security frameworks increasingly require explainability to ensure both compliance with laws/regulations and the safety of critical AI applications.</font></p></div></li></ul></li>
	<li class="col" id="FMID_737765156FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Privacy and Data Protection</div>
		<ul class="subexp">
	<li class="basic" id="FMID_346858286FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Because AI systems are often trained on large, sensitive datasets (including personal data), <strong>data protection</strong> is a central pillar of AI security<u><a href="https://learn.microsoft.com/en-us/training/paths/ai-security-fundamentals/" target="_blank">1</a></u>:</font></p><ul><li><p class="western" align="left"><font color="#000000">Secure data storage and access controls prevent unauthorized access.</font></p></li><li><p class="western" align="left"><font color="#000000">Data anonymization and privacy-preserving techniques (like differential privacy) reduce risk.</font></p></li><li><p class="western" align="left"><font color="#000000">Compliance with legal and regulatory standards (such as GDPR, HIPAA) is necessary to ensure ethical use of personal information and avoid severe penalties.</font></p></li></ul><p class="western" align="left"><font color="#000000">Protecting training data, inference-time data, and model parameters from exposure is critical, as adversaries may exploit weaknesses here to compromise security, privacy, or competitive advantage.</font></p></div></li></ul></li></ul></li>
	<li class="exp" id="FMID_1345723977FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Foundations of AI Security</div>
		<ul class="sub">
	<li class="col" id="FMID_50322583FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Definition and Scope: What Is AI Security?</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1641200530FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><strong><font color="#000000">AI Security</font></strong><font color="#000000"> refers to the set of principles, practices, and technologies designed to protect artificial intelligence (AI) systems against attacks, misuse, and unauthorized manipulation. This encompasses the security of the <strong>data</strong>, <strong>models</strong>, and <strong>infrastructure</strong> used by AI, ensuring they remain trustworthy, robust, and resistant to exploitation. The domain includes:</font></p><ul><li><p class="western" align="left"><font color="#000000">Protecting AI from adversarial attacks (where inputs are intentionally modified to mislead models).</font></p></li><li><p class="western" align="left"><font color="#000000">Safeguarding sensitive training or operational data from theft, tampering, or misuse.</font></p></li><li><p class="western" align="left"><font color="#000000">Preventing unauthorized access to AI models, protecting intellectual property and confidentiality.</font></p></li><li><p class="western" align="left"><font color="#000000">Ensuring the integrity and reliability of AI behavior under adversarial conditions.</font></p></li></ul><p class="western" align="left"><font color="#000000">The scope extends to managing AI systems across their <strong>entire lifecycle</strong>, from design to deployment, continually assessing and mitigating new risks as attack methods evolve.</font></p></div></li></ul></li>
	<li class="col" id="FMID_532921144FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Importance and Unique Challenges in AI Systems</div>
		<ul class="subexp">
	<li class="basic" id="FMID_614887999FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">AI's increasing integration into critical domains—like healthcare, finance, and national infrastructure—makes its security particularly vital. Unlike traditional software, AI poses unique challenges:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Non-determinism:</font></strong><font color="#000000"> Many AI models, especially those based on machine learning, can produce different outputs for similar or slightly altered inputs. Attackers can exploit this unpredictability.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Dependence on data:</font></strong><font color="#000000"> The performance and security of AI heavily rely on the quality and security of its training and operational data. Attacks on data (e.g., poisoning or manipulation) can subtly degrade or hijack model functionality.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Adversarial Vulnerabilities:</font></strong><font color="#000000"> Small, often imperceptible input modifications can cause catastrophic model failures—an issue largely unheard of in traditional systems.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Opaque decision-making:</font></strong><font color="#000000"> Complex AI models (such as deep learning networks) are difficult to interpret, making it challenging to verify correct behavior or detect manipulation<u><a href="https://arxiv.org/html/2504.16110v1" target="_blank">6</a></u>.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Evolving threat landscape:</font></strong><font color="#000000"> AI security is not static; threat actors continuously develop new attack techniques, necessitating ongoing adaptation and monitoring.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_66062836FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">AI Security vs. Traditional Cybersecurity</div>
		<ul class="subexp">
	<li class="basic" id="FMID_459354437FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><table width="643" cellpadding="2" cellspacing="0"><col width="119" /><col width="182" /><col width="330" /><tr><th width="119" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="center">
            Aspect
          </p></th><th width="182" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="center">
            Traditional Cybersecurity
          </p></th><th width="330" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="center">
            AI Security
          </p></th></tr><tr><td width="119" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Detection Approach
          </p></td><td width="182" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Rule-/Signature-based
          </p></td><td width="330" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Data-driven, anomaly/pattern detection
          </p></td></tr><tr><td width="119" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Target
          </p></td><td width="182" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Software, networks, endpoints
          </p></td><td width="330" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Data, models, pipelines, training process
          </p></td></tr><tr><td width="119" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Response
          </p></td><td width="182" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Largely reactive, manual
          </p></td><td width="330" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Automated, real-time, adaptive
          </p></td></tr><tr><td width="119" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Adaptability
          </p></td><td width="182" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Limited to known threats
          </p></td><td width="330" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Capable of adapting to new, unknown threats
          </p></td></tr><tr><td width="119" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Attack Types Managed
          </p></td><td width="182" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Malware, phishing, network intrusions
          </p></td><td width="330" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Adversarial attacks, data poisoning, model theft
          </p></td></tr><tr><td width="119" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Human Involvement
          </p></td><td width="182" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            High, routine updating required
          </p></td><td width="330" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western" align="left">
            Lower due to automation—but expert oversight needed for attacks on models and data
          </p></td></tr></table></div></li>
	<li class="basic" id="FMID_1172335562FM"><div class="nodecontent" style="color:#990000;font-size:117%;">Traditional cybersecurity uses static rules and relies on recognizing known attack signatures, making it effective against familiar threats but weak against novel or sophisticated attacks. By contrast, AI security leverages machine learningto predict, detect, and respond to evolving threats in real time, with enhanced automation and adaptability. However, AI systems introduce new attack surfaces and require dedicated methods for safeguarding model and data integrity.</div></li></ul></li>
	<li class="col" id="FMID_1502070941FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Security in AI vs. Security Using AI</div>
		<ul class="subexp">
	<li class="basic" id="FMID_421102060FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Security in AI</font></strong><font color="#000000"> refers to measures taken to <strong>protect AI systems themselves</strong> from being attacked or manipulated. Examples include defending against adversarial examples, data poisoning, and model extraction attacks.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Security using AI</font></strong><font color="#000000"> describes the use of <strong>AI techniques to enhance cybersecurity</strong>. This typically involves applying machine learning to analyze network traffic, detect anomalies, classify malware, or automate threat response—thus improving the security of digital systems in general.</font></p></li></ul><p class="western" align="left"><font color="#000000">These terms differentiate whether AI is the target of protection or a tool used to protect other assets. Securing AI is crucial as attackers may target or misuse AI systems, potentially causing widespread harm if such systems underpin critical infrastructure. Using AI for security gives defenders an advantage in rapidly identifying and neutralizing sophisticated cyber threats at scale.</font></p></div></li></ul></li>
	<li class="col" id="FMID_124734829FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Applications and Use Cases</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1545994188FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><strong><font color="#000000">Applications of AI Security span both defending AI systems and leveraging AI for broader defense:</font></strong></p><ul><li><p class="western" align="left"><strong><font color="#000000">Defending AI:</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Protecting facial recognition, autonomous vehicles, and natural language processing models from adversarial manipulation.</font></p></li><li><p class="western" align="left"><font color="#000000">Ensuring the integrity of medical decision support AI to prevent life-threatening tampering.</font></p></li><li><p class="western" align="left"><font color="#000000">Preserving privacy in AI-driven systems processing sensitive personal data by enforcing access controls, encryption, and anonymization.</font></p></li><li><p class="western" align="left"><font color="#000000">Safeguarding generative models (e.g., LLMs) from misuse (e.g., generating disinformation, automating phishing).</font></p></li></ul></li><li><p class="western" align="left"><strong><font color="#000000">Using AI in Cybersecurity:</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Automating detection of malware, fraud, or intrusions by spotting behavioral anomalies at scales unmanageable by humans.</font></p></li><li><p class="western" align="left"><font color="#000000">Real-time classification and segmentation of sensitive data for instant response to breaches.</font></p></li><li><p class="western" align="left"><font color="#000000">Defending networked environments with adaptive, self-learning detection systems.</font></p></li><li><p class="western" align="left"><font color="#000000">Enhancing incident response through automated triage and prioritization.</font></p></li></ul></li></ul><p class="western" align="left"><font color="#000000">Practical <strong>use cases</strong> include banking systems screening for fraudulent transactions, email providers filtering phishing attempts, cloud platforms automating vulnerability scanning, and critical infrastructure operators using anomaly detection to pre-empt operational disruptions.</font></p></div></li></ul></li></ul></li>
	<li class="exp" id="FMID_228165179FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Threat Landscape in AI Security</div>
		<ul class="sub">
	<li class="col" id="FMID_1303526767FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Traditional Cybersecurity Threats Affecting AI Systems</div>
		<ul class="subexp">
	<li class="basic" id="FMID_512075163FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">AI systems, by virtue of being built on software, networks, and data infrastructure, inherit many of the classic cybersecurity threats:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Network Intrusions</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Attackers exploit vulnerabilities in the network infrastructure hosting AI (e.g., servers, APIs, cloud instances), facilitating data theft, model extraction, or unauthorized manipulation.</font></p></li><li><p class="western" align="left"><font color="#000000">Tactics like ransomware, lateral movement across the network, and man-in-the-middle attacks threaten AI data pipelines and service endpoints.</font></p></li></ul></li><li><p class="western" align="left"><strong><font color="#000000">Unauthorized Access (to Models/Data)</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Poorly enforced access controls may allow attackers—external or internal—to retrieve trained AI models or sensitive datasets for analysis, theft, or later-stage attacks.</font></p></li><li><p class="western" align="left"><font color="#000000">Credential compromise and brute-force attacks can result in attackers gaining admin privileges over AI systems.</font></p></li></ul></li><li><p class="western" align="left"><strong><font color="#000000">Insider Threats</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Employees, contractors, or trusted partners misusing privileged access pose a severe risk, from intellectual property theft (exfiltrating proprietary models or training data) to sabotaging model behavior.</font></p></li><li><p class="western" align="left"><font color="#000000">Insider-facilitated attacks may evade basic monitoring, especially in organizations lacking robust auditing.</font></p></li></ul></li><li><p class="western" align="left"><strong><font color="#000000">Supply Chain Attacks</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Compromise of third-party libraries, pre-trained models, or hosted ML services can result in poisoned deliverables, backdoors, or malware introduced into otherwise secure AI pipelines.</font></p></li><li><p class="western" align="left"><font color="#000000">As AI relies on a vast ecosystem of open-source tools and data, the attack surface grows proportionally.</font></p></li></ul></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1521002746FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">AI-Specific Threats and Vulnerabilities</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1005939800FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Adversarial Attacks</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Attackers craft small, targeted input manipulations (adversarial examples) that trigger erroneous or dangerous outputs from models, such as causing misclassification in computer vision or bypassing anomaly detection.</font></p></li><li><p class="western" align="left"><font color="#000000">Such perturbations are often imperceptible to humans but can completely override AI decision logic.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1800518899FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Evasion Attacks</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Attackers systematically probe and manipulate inputs at inference time so that malicious activities are intentionally missed by classifiers (e.g., malware that evades detection).</font></p></li><li><p class="western" align="left"><font color="#000000">These attacks target models’ blind spots uncovered by exploratory testing against deployed endpoints.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_355189836FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Poisoning Attacks</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">By tainting training data—either inserting malicious samples or modifying legitimate ones—attackers can force AI to behave incorrectly or embed hidden triggers (“backdoors”).</font></p></li><li><p class="western" align="left"><font color="#000000">Compromised data sources, third-party datasets, or weak pipeline controls increase exposure.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_42239578FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Model Inversion and Membership Inference</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Attackers exploit model outputs to reconstruct or infer properties of the training data, violating data privacy.</font></p></li><li><p class="western" align="left"><font color="#000000">Membership inference attacks determine whether specific data points were used to train a model, risking exposure of sensitive or personal information.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1340402254FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Data Privacy Risks</font></strong></p><ul><li><p class="western" align="left"><em><font color="#000000">Leakage</font></em><font color="#000000">: AI models, especially large language models and generative systems, may inadvertently output private training examples, trade secrets, or confidential records.</font></p></li><li><p class="western" align="left"><em><font color="#000000">Biased Datasets</font></em><font color="#000000">: Training on unrepresentative or skewed data propagates systemic bias, creating discriminatory outcomes in high-stakes areas (e.g., finance, healthcare, hiring).</font></p></li><li><p class="western" align="left"><em><font color="#000000">Sensitive Data Exposure</font></em><font color="#000000">: Weak data governance during development, testing, or deployment can expose confidential data to unauthorized users.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1060156516FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Model and Intellectual Property Risks</font></strong></p><ul><li><p class="western" align="left"><em><font color="#000000">Theft and Reverse Engineering</font></em><font color="#000000">: Attackers can clone proprietary models using “model extraction” techniques or reconstruct designs from API access.</font></p></li><li><p class="western" align="left"><em><font color="#000000">Manipulation</font></em><font color="#000000">: Model logic can be modified by insiders or attackers with deployment access, enabling data leakage, sabotage, or financial harm.</font></p></li><li><p class="western" align="left"><em><font color="#000000">Abuse</font></em><font color="#000000">: Misuse of generative models to produce harmful or illegal content (e.g., deepfakes, disinformation) poses societal risks.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1949341580FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Deployment Risks</font></strong></p><ul><li><p class="western" align="left"><em><font color="#000000">API Abuse</font></em><font color="#000000">: Public-facing AI endpoints become targets for automated, large-scale probing (e.g., “prompt injection” in LLMs, excessive quota use), extracting sensitive functionality or triggering edge-case failures.</font></p></li><li><p class="western" align="left"><em><font color="#000000">Shadow Models</font></em><font color="#000000">: Unapproved or “shadow” AI deployments escape regular audits and governance, increasing the risk of insecure or non-compliant use.</font></p></li><li><p class="western" align="left"><em><font color="#000000">Unauthorized Model Access</font></em><font color="#000000">: Lack of authentication or insufficient logging allows attackers to interact with, manipulate, or extract information from production AI models.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1098009398FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Robustness and Reliability Concerns</font></strong></p><ul><li><p class="western" align="left"><em><font color="#000000">Distribution Shift</font></em><font color="#000000">: AI systems trained on historical data may degrade or fail when deployed in environments markedly different from their original context (e.g., new sensor, user behaviors, or adversarial environments).</font></p></li><li><p class="western" align="left"><em><font color="#000000">Out-of-Distribution (OOD) Data</font></em><font color="#000000">: Inputs vastly different from training data result in unreliable or unpredictable behavior, sometimes producing unsafe or biased outputs.</font></p></li><li><p class="western" align="left"><em><font color="#000000">Unexpected Failures</font></em><font color="#000000">: Over-reliance on AI systems, with weak monitoring or human oversight, can lead to catastrophic operational failures if models encounter unforeseen scenarios.</font></p></li></ul></li></ul></div></li></ul></li></ul></li>
	<li class="exp" id="FMID_1403572739FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Adversarial Machine Learning</div>
		<ul class="sub">
	<li class="col" id="FMID_644303784FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Fundamentals and Concepts</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1751115702FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><strong><font color="#000000">Adversarial Machine Learning (AML)</font></strong><font color="#000000"> is the study of vulnerabilities in machine learning models caused by intentionally crafted inputs designed to deceive the model into making incorrect decisions. These inputs, known as <strong>adversarial examples</strong>, often contain small perturbations that are imperceptible to humans but cause the AI model to malfunction, leading to wrong predictions or revealing sensitive information.</font></p><p class="western" align="left"><font color="#000000">AML serves two purposes: studying how attackers manipulate models and developing defense mechanisms to make models more robust. Adversarial attacks threaten safety, privacy, and trust in AI systems, especially in critical domains such as healthcare, finance, and autonomous systems.</font></p></div></li></ul></li>
	<li class="col" id="FMID_1637161908FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Types of Attacks</div>
		<ul class="subexp">
	<li class="basic" id="FMID_692756857FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><h2 class="western" align="left" style="line-height: 115%; margin-top: 0in; margin-bottom: 0.1in"><font color="#000000">White-box Attacks</font></h2><p class="western" align="left"><font color="#000000">In <strong>white-box attacks</strong>, the attacker has full knowledge of the model architecture, parameters, and training data. This comprehensive access allows crafting highly effective adversarial examples by leveraging model gradients. A canonical example is the <strong>Fast Gradient Sign Method (FGSM)</strong>, where small perturbations are added to inputs by calculating the gradient of the loss with respect to the input, tricking the model into misclassification.</font></p><p class="western" align="left"><font color="#000000">Mathematically, for an input xx, model parameters θθ, true label yy, loss function JJ, and a small magnitude ϵϵ,</font></p><p class="western" align="left"><font color="#000000">advx=x+ϵ⋅sign(∇xJ(θ,x,y))advx=x+ϵ⋅sign(∇xJ(θ,x,y)) </font></p><p class="western" align="left" /><p class="western" align="left"><font color="#000000">This technique efficiently generates adversarial inputs that exploit neural network vulnerabilities to linear perturbations.</font></p></div></li>
	<li class="basic" id="FMID_901422541FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><h2 class="western" align="left" style="line-height: 115%; margin-top: 0in; margin-bottom: 0.1in"><font color="#000000">Black-box Attacks</font></h2><p class="western" align="left"><font color="#000000">In <strong>black-box attacks</strong>, the attacker has no access to model internals and can only query the model to observe outputs. Attackers use these observations to build surrogate models that approximate the target and then generate adversarial inputs transferable to the original model. Techniques include finite-difference approximations and heuristic probing. These attacks are more realistic in deployed settings where model details are hidden.</font></p></div></li></ul></li>
	<li class="col" id="FMID_461068563FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Defense Strategies</div>
		<ul class="subexp">
	<li class="basic" id="FMID_27967698FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><h2 class="western" align="left" style="line-height: 115%; margin-top: 0in; margin-bottom: 0.1in"><font color="#000000">Adversarial Training</font></h2><p class="western" align="left"><font color="#000000">This approach augments training datasets with adversarial examples, teaching the model to correctly classify or reject perturbed inputs. Although effective for specific attack types, it increases training time and may overfit to known attack patterns.</font></p></div></li>
	<li class="basic" id="FMID_971019567FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><h2 class="western" align="left" style="line-height: 115%; margin-top: 0in; margin-bottom: 0.1in"><font color="#000000">Input Sanitization</font></h2><p class="western" align="left"><font color="#000000">Preprocessing inputs to remove or reduce adversarial noise helps defend models. Methods include filtering, denoising autoencoders, or feature squeezing to restrict input complexity, reducing attack success rates.</font></p></div></li>
	<li class="basic" id="FMID_602781572FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><h2 class="western" align="left" style="line-height: 115%; margin-top: 0in; margin-bottom: 0.1in"><font color="#000000">Certified Defenses</font></h2><p class="western" align="left"><font color="#000000">These methods provide mathematical guarantees (certificates) that a model's prediction will remain stable within a certain perturbation range of the input. Approaches include randomized smoothing and Lipschitz-continuity constraints, offering provable robustness.</font></p></div></li>
	<li class="basic" id="FMID_1769392525FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><h2 class="western" align="left" style="line-height: 115%; margin-top: 0in; margin-bottom: 0.1in"><font color="#000000">Defensive Distillation</font></h2><p class="western" align="left"><font color="#000000">Defensive distillation involves training a secondary model on softened outputs of an original model, reducing sensitivity to input perturbations. This technique smooths decision boundaries, making it harder for adversarial attacks to cause misclassification.</font></p></div></li>
	<li class="basic" id="FMID_1492992015FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><h2 class="western" align="left" style="line-height: 115%; margin-top: 0in; margin-bottom: 0.1in"><font color="#000000">Detection of Adversarial Inputs</font></h2><p class="western" align="left"><font color="#000000">Techniques here attempt to identify inputs that exhibit characteristics of adversarial examples before they reach the model. Detection methods include monitoring statistical outliers, input reconstruction errors, or auxiliary classifiers. These approaches aim to flag or discard suspicious inputs to maintain system integrity<u><a href="https://www.tutorialspoint.com/machine_learning/machine_learning_adversarial.htm" target="_blank">5</a></u>.</font></p><p class="western" align="left"><font color="#000000">This chapter provides a foundational understanding of adversarial machine learning, explaining the fundamental concepts, distinctions between white-box and black-box attacks, and key defense mechanisms designed to enhance AI security. If you wish, I can also provide practical examples, code snippets, or discussion questions for students.</font></p></div></li></ul></li></ul></li>
	<li class="exp" id="FMID_1246227800FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Privacy-Preserving and Secure Machine Learning Techniques</div>
		<ul class="sub">
	<li class="col" id="FMID_121870243FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Introduction</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1653538978FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">As machine learning (ML) increasingly permeates sensitive domains such as healthcare, finance, and government, ensuring the privacy and security of data used in ML processes has become paramount. The nature of ML — often reliant on vast and detailed datasets — raises significant privacy risks, including unauthorized data inference and re-identification. To address these challenges, a suite of <strong>privacy-preserving and secure ML techniques</strong> have been developed and actively researched, enabling trustworthy AI deployments without compromising data confidentiality. </font></p><p class="western" align="left" /><p class="western" align="left"><font color="#000000">This chapter introduces several foundational methods: <strong>Differential Privacy</strong>, <strong>Federated Learning Security</strong>, <strong>Homomorphic Encryption</strong>, <strong>Secure Multi-Party Computation</strong>, and the use of <strong>Data Encryption and Access Controls</strong>in secure machine learning environments.</font></p></div></li></ul></li>
	<li class="col" id="FMID_1536463681FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Differential Privacy</div>
		<ul class="subexp">
	<li class="col" id="FMID_714587777FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><strong><font color="#000000">Differential Privacy (DP)</font></strong><font color="#000000"> is a rigorous mathematical framework designed to provide quantifiable privacy guarantees when analyzing or sharing aggregate data. The core objective of differential privacy is to ensure that the inclusion or exclusion of a single data point (individual's record) in a dataset does not significantly affect the output of any analysis, making it infeasible for adversaries to infer information about any individual.</font></p><p /><p class="western" align="left"><font color="#000000">Researchers continue to expand the capabilities of DP, applying it for fairness, robustness, and security enhancement in AI systems beyond mere privacy protection.</font></p></div>
		<ul class="subexp">
	<li class="basic" id="FMID_506019216FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Mechanism</font></strong><font color="#000000">: DP typically introduces random noise, drawn from specific distributions (e.g., Laplace or Gaussian), into query results or model parameters, thereby obscuring the contribution of individual data points while preserving overall statistical properties.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_1682269036FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Applications in ML</font></strong><font color="#000000">: Through mechanisms like <strong>differentially private stochastic gradient descent (DP-SGD)</strong>, ML models can be trained on sensitive data with provable privacy, allowing organizations to leverage data insights without direct exposure. The noise addition balances privacy with utility, as excessive noise may degrade model accuracy.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_262388790FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Advantages</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Provides strong privacy assurances resilient against sophisticated attacks.</font></p></li><li><p class="western" align="left"><font color="#000000">Facilitates compliance with regulations such as GDPR.</font></p></li><li><p class="western" align="left"><font color="#000000">Enables safe data sharing and collaborative research via synthetic data generation.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1489498707FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Challenges</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Trade-offs between privacy and accuracy.</font></p></li><li><p class="western" align="left"><font color="#000000">Increased computational overhead.</font></p></li><li><p class="western" align="left"><font color="#000000">Complex parameter tuning to achieve desired privacy levels.</font></p></li></ul></li></ul></div></li></ul></li></ul></li>
	<li class="col" id="FMID_1806966702FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Federated Learning Security</div>
		<ul class="subexp">
	<li class="col" id="FMID_453389163FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><strong><font color="#000000">Federated Learning (FL)</font></strong><font color="#000000"> is a decentralized ML approach where multiple parties collaboratively train a shared model while retaining their private data locally. Instead of sending raw data to a central server, each participant computes model updates locally, exchanging only those updates for aggregation.</font></p><p /><p class="western" align="left"><font color="#000000">FL stands as a critical paradigm enabling privacy-preserving ML at scale, especially where regulatory and ethical constraints limit direct data sharing.</font></p></div>
		<ul class="subexp">
	<li class="basic" id="FMID_282449169FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Privacy Benefits</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Raw data never leaves its origin, reducing exposure.</font></p></li><li><p class="western" align="left"><font color="#000000">Supports use cases involving sensitive data scattered across devices or institutions (e.g., hospitals, smartphones).</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1229320182FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Security Challenges</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Vulnerability to various attacks such as poisoning (manipulating data to corrupt models), inference attacks (extracting private information from updates), and backdoor attacks.</font></p></li><li><p class="western" align="left"><font color="#000000">Ensuring trustworthiness of participating nodes and integrity of updates.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1676355320FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Enhancements for Privacy</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">FL systems often integrate differential privacy to add noise to model updates, further preventing leakage about individual data.</font></p></li><li><p class="western" align="left"><font color="#000000">Cryptographic techniques like homomorphic encryption secure communication and aggregation.</font></p></li><li><p class="western" align="left"><font color="#000000">Mechanisms such as secure aggregation protocols prevent disclosure of individual updates while allowing accurate global model synthesis.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_895542092FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">FL Variants</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Horizontal FL (common features, different samples),</font></p></li><li><p class="western" align="left"><font color="#000000">Vertical FL (different features, same samples),</font></p></li><li><p class="western" align="left"><font color="#000000">Federated Transfer Learning.</font></p></li></ul></li></ul></div></li></ul></li></ul></li>
	<li class="col" id="FMID_980678971FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Homomorphic Encryption</div>
		<ul class="subexp">
	<li class="col" id="FMID_202572716FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><strong><font color="#000000">Homomorphic Encryption (HE)</font></strong><font color="#000000"> is a powerful cryptographic technique that permits computations on encrypted data without needing decryption, thus preserving data confidentiality throughout processing.</font></p><p /><p class="western" align="left"><font color="#000000">HE, combined with other privacy techniques, represents a cornerstone of secure ML, allowing collaborative and outsourced computations without data exposure.</font></p></div>
		<ul class="subexp">
	<li class="basic" id="FMID_271342331FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Types</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><em><font color="#000000">Partially Homomorphic Encryption (PHE)</font></em><font color="#000000">: Supports only one operation type (addition or multiplication).</font></p></li><li><p class="western" align="left"><em><font color="#000000">Somewhat Homomorphic Encryption (SHE)</font></em><font color="#000000">: Supports limited operations.</font></p></li><li><p class="western" align="left"><em><font color="#000000">Fully Homomorphic Encryption (FHE)</font></em><font color="#000000">: Supports arbitrary computations on ciphertexts, though with notable computational costs.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_307051603FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Role in ML</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Enables cloud or third-party ML services to perform inference or training on data still encrypted, mitigating risks of data leakage.</font></p></li><li><p class="western" align="left"><font color="#000000">Supports secure aggregation in federated learning.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1401983719FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Practical Considerations</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">HE schemes often introduce computational overhead and latency.</font></p></li><li><p class="western" align="left"><font color="#000000">Advances have improved efficiency, making HE increasingly viable for specific ML workloads, especially where privacy is critical (e.g., healthcare, finance).</font></p></li></ul></li></ul></div></li></ul></li></ul></li>
	<li class="col" id="FMID_706109082FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Secure Multi-Party Computation (SMPC)</div>
		<ul class="subexp">
	<li class="col" id="FMID_210405619FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><strong><font color="#000000">Secure Multi-Party Computation</font></strong><font color="#000000"> is a cryptographic protocol where multiple parties jointly compute a function over their inputs while keeping those inputs private.</font></p><p /><p class="western" align="left"><font color="#000000">SMPC can be combined with federated learning and differential privacy to build robust privacy-preserving ML frameworks.</font></p></div>
		<ul class="subexp">
	<li class="basic" id="FMID_157660991FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Concept</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Parties each hold private data.</font></p></li><li><p class="western" align="left"><font color="#000000">Through cryptographic protocols, they compute a joint function output without revealing their inputs to each other.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1758466215FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Applications in ML</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Collaborative training of joint models without sharing raw data.</font></p></li><li><p class="western" align="left"><font color="#000000">Performing privacy-preserving inference.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_565799302FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Advantages</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Strong theoretical privacy guarantees.</font></p></li><li><p class="western" align="left"><font color="#000000">No reliance on a trusted central party.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1502285440FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Challenges</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Communication and computation complexity.</font></p></li><li><p class="western" align="left"><font color="#000000">Scalability with many participants.</font></p></li></ul></li></ul></div></li></ul></li></ul></li>
	<li class="col" id="FMID_1896143664FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Data Encryption and Access Controls</div>
		<ul class="subexp">
	<li class="col" id="FMID_885948029FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">At the foundational level, <strong>data encryption and access control mechanisms</strong> are essential in secure ML pipelines to protect data at rest and in transit.</font></p><p /><p class="western" align="left"><font color="#000000">These controls are necessary complements to advanced cryptographic and algorithmic privacy techniques to form an end-to-end secure ML ecosystem.</font></p></div>
		<ul class="subexp">
	<li class="basic" id="FMID_1664172695FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Encryption</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Standard encryption algorithms (AES, RSA) are used to secure datasets stored in databases or cloud storage.</font></p></li><li><p class="western" align="left"><font color="#000000">Transport Layer Security (TLS) secures communication channels.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1929215995FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Access Controls</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Role-based and attribute-based access control systems restrict data access to authorized users.</font></p></li><li><p class="western" align="left"><font color="#000000">Auditing and monitoring track data access patterns to detect anomalies.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1355860687FM"><div class="nodecontent" style="color:#111111;"><ul><li><p class="western" align="left"><strong><font color="#000000">Integration</font></strong><font color="#000000">:</font></p><ul><li><p class="western" align="left"><font color="#000000">Combining access controls with encryption ensures multi-layered data protection.</font></p></li><li><p class="western" align="left"><font color="#000000">Supports compliance with data privacy laws and internal policies.</font></p></li></ul></li></ul></div></li></ul></li></ul></li></ul></li>
	<li class="exp" id="FMID_10352045FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Model Security and Robustness in Machine Learning</div>
		<ul class="sub">
	<li class="col" id="FMID_693956297FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Introduction</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1469715469FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Machine learning (ML) models have become core components in many critical areas such as healthcare, finance, cybersecurity, and autonomous systems. Consequently, <strong>model security and robustness</strong> have emerged as essential requirements to ensure these models operate reliably and safely under various conditions including adversarial attempts, data shifts, and unforeseen inputs. This chapter explores techniques to <strong>secure, harden, explain, monitor, and verify</strong> ML models to foster trustworthy AI systems.</font></p></div></li></ul></li>
	<li class="col" id="FMID_278883686FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Secure Model Deployment</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1048715503FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Secure deployment involves protecting ML models and their infrastructures from unauthorized access, tampering, and exploitation. Key considerations include:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Access control:</font></strong><font color="#000000"> Restrict model access using authentication, authorization, and role-based permissions to prevent abuse.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Secure API endpoints:</font></strong><font color="#000000"> Harden model serving endpoints against injection, tampering, and denial-of-service attacks.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Environment isolation:</font></strong><font color="#000000"> Use containerization and virtualization to isolate model execution from other system components.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Encryption:</font></strong><font color="#000000"> Protect model binaries and communication channels (e.g., via TLS) to safeguard confidentiality and integrity.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Logging and auditing:</font></strong><font color="#000000"> Maintain detailed logs of inference requests and administrative actions for accountability and forensic analysis.</font></p></li></ul><p class="western" align="left"><font color="#000000">Secure deployment reduces the attack surface, ensuring adversaries cannot easily compromise the model or its data during live operations.</font></p></div></li></ul></li>
	<li class="col" id="FMID_1258091792FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Model Hardening Strategies</div>
		<ul class="subexp">
	<li class="basic" id="FMID_930990095FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">To increase resilience against attacks and performance degradation, model hardening employs several strategies:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Compression:</font></strong><font color="#000000"> Techniques like pruning, quantization, and knowledge distillation reduce model complexity and size, which can sometimes improve robustness by discarding redundant or noisy parameters.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Noise Injection:</font></strong><font color="#000000"> Adding carefully designed noise during training or inference (e.g., Gaussian noise, dropout) helps models generalize better and resist adversarial perturbations.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Regularization:</font></strong><font color="#000000"> Approaches such as L1/L2 regularization, weight decay, and adversarial training prevent overfitting and improve robustness, especially against adversarial examples crafted to mislead models.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Adversarial Training:</font></strong><font color="#000000"> Including adversarially modified inputs during training teaches the model to recognize and correctly classify perturbed instances, directly enhancing resistance to evasion attacks.</font></p></li></ul><p class="western" align="left"><font color="#000000">These strategies collectively improve a model's capacity to maintain performance amid attacks and data uncertainties.</font></p></div></li></ul></li>
	<li class="col" id="FMID_191856672FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Explainability and Interpretability for Auditing</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1900232326FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Understanding and auditing ML models is vital for verifying their reliability and detecting suspicious behavior:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Explainability techniques:</font></strong><font color="#000000"> Methods such as SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations), and saliency maps reveal the influence of input features on predictions.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Interpretability:</font></strong><font color="#000000"> Simplifying or structuring models in ways humans can understand (e.g., decision trees, rule-based models) aids auditors in validating decision logic.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Auditing use cases:</font></strong><font color="#000000"> Explainability helps detect bias, fairness issues, and anomalies indicative of attacks like data poisoning or backdoors.</font></p></li></ul><p class="western" align="left"><font color="#000000">By making models transparent, explainability contributes to security by enabling informed oversight and debugging.</font></p></div></li></ul></li>
	<li class="col" id="FMID_786974453FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Model Monitoring and Auditing</div>
		<ul class="subexp">
	<li class="basic" id="FMID_133632332FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Continuous oversight of models in production is critical to detect degradation or attacks early:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Performance monitoring:</font></strong><font color="#000000"> Track accuracy, precision, recall, and other metrics to notice sudden changes that may indicate data drift or adversarial interference.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Data drift and concept drift detection:</font></strong><font color="#000000"> Identify shifts in input data distribution or label relationships which can degrade model validity.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Anomaly detection:</font></strong><font color="#000000"> Employ statistical methods or ML for spotting outlier inputs or anomalous prediction patterns suggestive of attacks.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Audit trails:</font></strong><font color="#000000"> Log predictions, inputs, and system changes for compliance and forensic analysis.</font></p></li></ul><p class="western" align="left"><font color="#000000">An effective monitoring framework enables quick mitigation and recovery from security or robustness incidents.</font></p></div></li></ul></li>
	<li class="col" id="FMID_1473583810FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Formal Verification and Robustness Testing</div>
		<ul class="subexp">
	<li class="basic" id="FMID_319665638FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Formal methods apply mathematical techniques to guarantee ML model properties and discover vulnerabilities:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Formal Verification:</font></strong><font color="#000000"> Use program analysis and model checking tools to prove that models meet robustness criteria (e.g., bounded output perturbations given bounded input perturbations). This can detect worst-case adversarial examples and certify safety margins.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Robustness Testing:</font></strong><font color="#000000"> Simulate adversarial attacks and data shifts to empirically assess model resilience. Techniques include:</font></p><ul><li><p class="western" align="left"><font color="#000000">Adversarial attacks (FGSM, PGD, CW)</font></p></li><li><p class="western" align="left"><font color="#000000">Random perturbations and noise injection</font></p></li><li><p class="western" align="left"><font color="#000000">Testing on out-of-distribution data sets</font></p></li></ul></li></ul><p class="western" align="left"><font color="#000000">Formal verification provides provable guarantees, while robustness testing offers practical insights into model defensive strengths and weaknesses.</font></p></div></li></ul></li>
	<li class="col" id="FMID_1875109164FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Handling Uncertainty and Out-of-Distribution (OOD) Detection</div>
		<ul class="subexp">
	<li class="basic" id="FMID_814884433FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">ML models must recognize when they are uncertain or face unfamiliar inputs, crucial for safety in real-world applications:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Uncertainty quantification:</font></strong><font color="#000000"> Methods like Bayesian neural networks, Monte Carlo dropout, and ensemble modeling estimate prediction confidence, helping avoid overconfident errors.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Out-of-Distribution detection:</font></strong><font color="#000000"> Identify inputs deviating significantly from training distribution using techniques such as:</font></p><ul><li><p class="western" align="left"><font color="#000000">Density estimation (e.g., Gaussian Mixture Models)</font></p></li><li><p class="western" align="left"><font color="#000000">Distance-based measures in feature space</font></p></li><li><p class="western" align="left"><font color="#000000">Specialized neural network layers for OOD detection</font></p></li></ul></li></ul><p class="western" align="left"><font color="#000000">Recognizing uncertainty and OOD inputs prevents critical failures and guides fallback or human intervention mechanisms.</font></p></div></li></ul></li></ul></li>
	<li class="exp" id="FMID_1186721951FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Security in the Machine Learning Lifecycle</div>
		<ul class="sub">
	<li class="col" id="FMID_1334912055FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Introduction</div>
		<ul class="subexp">
	<li class="basic" id="FMID_981976553FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">The machine learning (ML) lifecycle encompasses a series of stages including data collection, model training, evaluation, deployment, and inference. Ensuring <strong>security at every step</strong> is critical to protecting sensitive data, maintaining model integrity, and defending against adversarial threats. This chapter covers the best practices for securing the ML lifecycle, focusing on key areas such as source validation, secure environments, adversarial resilience, robustness testing, and runtime protections.</font></p></div></li></ul></li>
	<li class="col" id="FMID_1675990575FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Data Collection</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1895002763FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Securing the data collection phase is foundational because <strong>data quality and trustworthiness</strong> directly impact downstream model security.</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Source Validation</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Validate and verify the sources of collected data to avoid contaminated, malicious, or poisoned data inputs. Use provenance tracking and perform sanity checks on data to confirm authenticity and integrity before use.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Data Sanitization</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Cleanse data by removing or anonymizing sensitive information (such as personally identifiable information, PII) and filtering out corrupted or adversarially crafted samples. Techniques include anomaly detection on raw data and applying rigorous preprocessing pipelines to detect and discard suspicious entries.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Minimizing Data Exposure</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Limit data access to only authorized personnel and tools, enforcing the principle of least privilege. Employ encryption both at rest and in transit to protect data confidentiality throughout its lifecycle.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1105234873FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Model Training</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1125537761FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><a name="__DdeLink__2665_1890289260" /><font color="#000000">Training environments must be <strong>secure, reliable, and resilient</strong> to adversarial attempts.</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Secure Environments</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Use isolated and hardened compute environments, such as secured containers or trusted execution environments (TEEs), to prevent unauthorized access or tampering of training data and code.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Adversarial Resilience</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Incorporate adversarial training techniques where models are trained on both clean and adversarially perturbed examples to improve robustness against evasion attempts. Use differential privacy or other privacy-preserving methods to prevent leakage of individual training data.</font></p></li><li><p class="western" align="left"><a name="__DdeLink__2669_1890289260" /><strong><font color="#000000">Version Control and Experiment Tracking</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Employ rigorous versioning of training code, model artifacts, and datasets to ensure traceability and auditability, enabling rollback and forensic analysis in case of security incidents.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_880853677FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Model Evaluation</div>
		<ul class="subexp">
	<li class="basic" id="FMID_719215941FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Secure evaluation verifies model robustness and detects hidden vulnerabilities.</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Robustness Testing</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Conduct extensive testing against adversarial attacks (e.g., FGSM, PGD techniques) and simulate real-world perturbations to assess model performance under hostile conditions.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Red Teaming</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Employ dedicated teams or automated tools to simulate sophisticated attacks aimed at uncovering security flaws. Red teaming helps identify weak points in models that standard testing might miss.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Bias and Fairness Audits</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Evaluate models for unintended bias or fairness issues that can also be security risks by exposing vulnerable populations to harm.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1862570997FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Deployment and Inference</div>
		<ul class="subexp">
	<li class="basic" id="FMID_435896166FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Securing model deployment and inference safeguards model integrity and prevents misuse.</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Access Control</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Implement strong authentication and authorization policies on model APIs. Use role-based access control (RBAC) to minimize who can query or modify models.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">API Security</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Harden inference endpoints against injection attacks, rate limiting to prevent denial-of-service (DoS), and validate all incoming requests to prevent exploitation.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Runtime Monitoring</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Continuously monitor live models for anomalies such as deviations in prediction distributions, unexpected input patterns, or abnormal response times. Deploy alerts for suspicious activity to enable swift incident response.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Encryption</font></strong><font color="#000000">:<br style="font-variant: normal; letter-spacing: normal" />Protect data in transit between clients and model services using protocols like TLS. Consider encrypting model binaries and weights to prevent intellectual property theft.</font></p></li></ul></div></li></ul></li></ul></li>
	<li class="exp" id="FMID_1936516748FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Security Governance, Frameworks, and Standards</div>
		<ul class="sub">
	<li class="col" id="FMID_776420129FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Introduction</div>
		<ul class="subexp">
	<li class="basic" id="FMID_761092136FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western"><font color="#000000">As Artificial Intelligence (AI) systems become increasingly integral to organizational operations, securing these systems requires a structured approach encompassing governance, standards, and frameworks. A robust <strong>AI Security Governance</strong> framework ensures not only protection against risks but also compliance with evolving regulations and ethical mandates. This chapter explores the essential elements of AI security governance, organizational roles, common frameworks and standards, risk assessment processes, and the concept of a Secure AI Development Lifecycle.</font></p></div></li>
	<li class="basic" id="FMID_1394704753FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><span style="color: #00b439; font-size: 133%; font-family: SansSerif, sans-serif"><font color="#00b439" size="133%" face="SansSerif, sans-serif"> </font></span><p class="western">
       <strong><font color="#000000">Security governance</font></strong><font color="#000000">is the organizational framework of responsibilities, policies, and decision-making processes that direct how AI security is managed to achieve strategic objectives while minimizing risks.</font></p><ul><li><p class="western">
           <strong><font color="#000000">Purpose:</font></strong><font color="#000000"> It aligns AI security initiatives with business goals, regulatory requirements, and ethical considerations.</font></p></li><li><p class="western">
           <strong><font color="#000000">Scope:</font></strong><font color="#000000"> Covers all AI systems, data, personnel, and third-party interactions across the AI lifecycle.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_455074335FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Security Governance in AI</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1814262032FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Organizational Policies and Governance Structures</b></font></p><p class="western"><font color="#000000">Effective governance begins with clear, documented <strong>policies</strong>and governance <strong>structures</strong>that define how AI security is managed:</font></p><ul><li><p class="western"><strong><font color="#000000">Policies</font></strong><font color="#000000"> set rules for AI use, data privacy, ethical standards, and risk tolerance.</font></p></li><li><p class="western"><strong><font color="#000000">Governance structures</font></strong><font color="#000000"> establish authority through committees or councils responsible for oversight, periodic reviews, and enforcement.</font></p></li><li><p class="western"><font color="#000000">Inclusion of <strong>cross-functional teams</strong> ensures diverse perspectives from legal, compliance, IT security, and AI development.</font></p></li></ul><p class="western"><font color="#000000">In 2025, organizations increasingly adopt governance models advocating continuous oversight, transparency, and accountability to address AI’s complexity and regulatory scrutiny.</font></p></div></li>
	<li class="basic" id="FMID_1193651513FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Roles, Responsibilities, and Cross-Functional Collaboration</b></font></p><p class="western"><font color="#000000">Security governance assigns <strong>clear roles and responsibilities</strong>:</font></p><ul><li><p class="western"><strong><font color="#000000">Executive leadership:</font></strong><font color="#000000"> Define AI security strategy and allocate resources.</font></p></li><li><p class="western"><strong><font color="#000000">AI security officers:</font></strong><font color="#000000"> Lead technical security, compliance, and risk management.</font></p></li><li><p class="western"><strong><font color="#000000">Data scientists and developers:</font></strong><font color="#000000"> Implement secure coding, model development, and testing.</font></p></li><li><p class="western"><strong><font color="#000000">Compliance and legal teams:</font></strong><font color="#000000"> Monitor adherence to laws such as GDPR, HIPAA, and the EU AI Act.</font></p></li><li><p class="western"><strong><font color="#000000">Operations and incident response teams:</font></strong><font color="#000000"> Manage deployment security and respond to incidents.</font></p></li></ul><p class="western"><font color="#000000">Cross-department collaboration is critical. Effective AI security integrates perspectives across:</font></p><ul><li><p class="western"><strong><font color="#000000">Security teams:</font></strong><font color="#000000"> Implement controls and monitor threats.</font></p></li><li><p class="western"><strong><font color="#000000">Ethics officers:</font></strong><font color="#000000"> Oversee bias mitigation and ethical AI use.</font></p></li><li><p class="western"><strong><font color="#000000">Business units:</font></strong><font color="#000000"> Align AI use cases with governance.</font></p></li></ul><p class="western"><font color="#000000">Studies show over 65% of governance failures arise from unclear roles or lack of collaboration, highlighting the need for well-structured, cooperative efforts.</font></p></div></li></ul></li>
	<li class="col" id="FMID_151010197FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Security and Risk Assessments</div>
		<ul class="subexp">
	<li class="basic" id="FMID_37223890FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western"><font color="#000000">An ongoing <strong>risk-based approach</strong> is fundamental to AI security governance.</font></p><ul><li><p class="western"><font color="#000000">Organizations must regularly conduct <strong>security and risk assessments</strong> to identify vulnerabilities, threats (e.g., adversarial attacks), and compliance gaps.</font></p></li><li><p class="western"><font color="#000000">These assessments evaluate the AI system’s lifecycle stages—from data collection to model deployment and monitoring.</font></p></li><li><p class="western"><font color="#000000">Aligning with frameworks like <strong>NIST AI Risk Management Framework (RMF)</strong> provides structured guidance on risk identification, measurement, and mitigation.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_150243803FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western"><font color="#000000"><b>Key risk areas include:</b></font></p><ul><li><p class="western"><font color="#000000">Data integrity and provenance</font></p></li><li><p class="western"><font color="#000000">Model robustness and adversarial resilience</font></p></li><li><p class="western"><font color="#000000">Deployment security and runtime monitoring</font></p></li><li><p class="western"><font color="#000000">Supply chain risks from third-party AI components</font></p></li></ul><p class="western"><font color="#000000">Risk assessments inform mitigation strategies, incident response planning, and governance priorities.</font></p></div></li></ul></li>
	<li class="col" id="FMID_1225817006FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Industry Standards for AI Security</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1003041459FM"><div class="nodecontent" style="color:#990000;font-size:117%;">The landscape of AI security standards is rapidly evolving, with international and regional bodies defining best practices.</div></li>
	<li class="basic" id="FMID_1340451290FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>NIST AI Risk Management Framework (RMF)</b></font></p><ul><li><p class="western"><font color="#000000">Published by the US National Institute of Standards and Technology, the <strong>NIST AI RMF</strong> guides organizations in managing AI risks with focus areas such as governance, transparency, fairness, and accountability.</font></p></li><li><p class="western"><font color="#000000">It uses core functions—<strong>Govern, Map, Measure, Manage</strong>—to systematize risk control throughout the AI lifecycle.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_1703463837FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>ISO/IEC AI and Cybersecurity Standards</b></font></p><ul><li><p class="western"><strong><font color="#000000">ISO/IEC 42001</font></strong><font color="#000000"> is the inaugural global standard dedicated to AI management systems, emphasizing governance, risk management, and security controls tailored for AI.</font></p></li><li><p class="western"><font color="#000000">It builds alignment with broader cybersecurity standards like <strong>ISO/IEC 27001</strong> for information security to foster consistent, organization-wide controls.</font></p></li><li><p class="western"><font color="#000000">Adoption of ISO/IEC standards aids compliance with regional regulations and cross-border audits.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_335398608FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Other Standards and Principles</b></font></p><ul><li><p class="western"><strong><font color="#000000">OECD AI Principles:</font></strong><font color="#000000"> Promote trustworthy AI emphasizing human rights, fairness, and transparency.</font></p></li><li><p class="western"><strong><font color="#000000">EU AI Act:</font></strong><font color="#000000"> Upcoming regulatory framework imposing stringent requirements on high-risk AI applications with heavy penalties for non-compliance.</font></p></li><li><p class="western"><strong><font color="#000000">Industry frameworks</font></strong><font color="#000000"> like the <strong>OWASP Top 10 for Large Language Models (LLMs)</strong> and the <strong>PEACH Model</strong> that target specific AI threat categories (e.g., prompt injection, model poisoning).</font></p></li></ul><p class="western"><font color="#000000">Adherence to these standards ensures organizations operate AI responsibly, legally, and securely.</font></p></div></li></ul></li>
	<li class="col" id="FMID_545946284FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Security Frameworks Specific to AI</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1132635452FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western"><font color="#000000">Security frameworks provide actionable blueprints for integrating security controls and managing AI threats.</font></p></div></li>
	<li class="basic" id="FMID_1264024984FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>OWASP Top 10 for LLMs</b></font></p><ul><li><p class="western"><font color="#000000">Extends OWASP’s classical web application risks to the emerging domain of <strong>Large Language Models</strong>.</font></p></li><li><p class="western"><font color="#000000">Highlights risks like <strong>prompt injection attacks</strong>, data leakage, and model manipulation.</font></p></li><li><p class="western"><font color="#000000">Offers mitigation techniques to safeguard model integrity and confidentiality.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_602776500FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>PEACH Model</b></font></p><ul><li><p class="western"><font color="#000000">Focuses on <strong>Privacy, Ethics, Accountability, Cybersecurity, and Human-Centric design</strong>.</font></p></li><li><p class="western"><font color="#000000">Encourages organizations to address AI risks holistically, beyond technical controls.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_710767038FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Integration with Traditional Security Frameworks</b></font></p><ul><li><p class="western"><font color="#000000">AI security frameworks often coexist with established cybersecurity models (e.g., NIST Cybersecurity Framework, MITRE ATT&amp;CK) adapted for AI’s unique risks.</font></p></li><li><p class="western"><font color="#000000">These frameworks guide controls such as access management, logging, and incident response specialized for AI environments.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_528782389FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Secure AI Development Lifecycle (SDLC for AI)</div>
		<ul class="subexp">
	<li class="basic" id="FMID_821776380FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western"><font color="#000000">The <strong>SDLC for AI</strong> adapts traditional software development security principles to the AI/ML context by embedding security and governance across all development phases:</font></p><table width="643" cellpadding="2" cellspacing="0"><col width="167" /><col width="467" /><tr><th width="167" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Phase
          </p></th><th width="467" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Key Security Activities
          </p></th></tr><tr><td width="167" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Data Collection</strong></p></td><td width="467" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Source validation, data sanitization, privacy-preserving data handling
          </p></td></tr><tr><td width="167" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Model Training</strong></p></td><td width="467" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Secure training environments, adversarial resilience methods, bias detection
          </p></td></tr><tr><td width="167" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Model Evaluation</strong></p></td><td width="467" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Robustness testing, red teaming, explainability assessment
          </p></td></tr><tr><td width="167" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Deployment &amp; Inference</strong></p></td><td width="467" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Access control, API security, runtime monitoring, anomaly detection
          </p></td></tr><tr><td width="167" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Maintenance</strong></p></td><td width="467" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Model updates, patching vulnerabilities, continuous risk monitoring
          </p></td></tr></table><ul><li><p class="western"><font color="#000000">This life-cycle approach ensures that AI models are designed, tested, and operated securely, mitigating risks such as data poisoning, evasion attacks, and unauthorized access.</font></p></li><li><p class="western"><font color="#000000">SDLC for AI enforces <strong>governance checkpoints</strong> at each phase to align development objectives with organizational policies and compliance requirements.</font></p></li></ul></div></li></ul></li></ul></li>
	<li class="exp" id="FMID_612368889FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Technical Defenses and Tools for AI Security</div>
		<ul class="sub">
	<li class="col" id="FMID_1814555161FM"><div class="nodecontent" style="color:#00b439;font-size:133%;"> Introduction</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1875505570FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">AI systems face unique and evolving threats, including adversarial attacks, data leaks, prompt manipulations, and model drift. Technical defenses combine detection, mitigation, secure environments, continuous monitoring, and specialized toolkits to protect AI throughout its lifecycle. Implementing defense-in-depth and aligning with zero trust principles are critical to resilient AI security.</font></p></div></li></ul></li>
	<li class="col" id="FMID_1370840185FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Adversarial Input Detection and Mitigation</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1433867843FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Adversarial Inputs</font></strong><font color="#000000"> refer to malicious inputs crafted to deceive AI models into incorrect or harmful outputs.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Detection Techniques:</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Statistical anomaly detection on input features.</font></p></li><li><p class="western" align="left"><font color="#000000">Use of confidence thresholds and out-of-distribution detection.</font></p></li><li><p class="western" align="left"><font color="#000000">Monitoring model behavior for unusual patterns.</font></p></li></ul></li><li><p class="western" align="left"><strong><font color="#000000">Mitigation Methods:</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Adversarial training by augmenting training data with adversarial examples.</font></p></li><li><p class="western" align="left"><font color="#000000">Input preprocessing to sanitize or normalize inputs.</font></p></li><li><p class="western" align="left"><font color="#000000">Gradient masking or defensive distillation to reduce model vulnerability.</font></p></li></ul></li><li><p class="western" align="left"><font color="#000000">Libraries such as <strong>IBM Adversarial Robustness Toolbox (ART)</strong> provide implementations of attacks and defenses for testing and hardening models.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_655050803FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Data Loss Prevention (DLP)</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1008044534FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><font color="#000000">DLP protects sensitive training data, inference outputs, and model artifacts from unauthorized access or leakage.</font></p></li><li><p class="western" align="left"><font color="#000000">Techniques include:</font></p><ul><li><p class="western" align="left"><font color="#000000">Encryption at rest and in transit.</font></p></li><li><p class="western" align="left"><font color="#000000">Tokenization and masking of sensitive fields before use.</font></p></li><li><p class="western" align="left"><font color="#000000">Access controls and audit logging on data repositories and model storage.</font></p></li><li><p class="western" align="left"><font color="#000000">Monitoring for anomalous data access or exfiltration.</font></p></li></ul></li><li><p class="western" align="left"><font color="#000000">Embedding DLP into AI pipelines ensures training data confidentiality and reduces risk from insider threats or accidental leaks.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1232597664FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Prompt Security in Large Language Models (LLMs)</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1484871507FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><font color="#000000">Prompt security involves guarding against <strong>Prompt Injection Attacks</strong>, where attackers craft inputs to manipulate or bypass LLM controls.</font></p></li><li><p class="western" align="left"><font color="#000000">Defenses include:</font></p><ul><li><p class="western" align="left"><font color="#000000">Input sanitization and validation.</font></p></li><li><p class="western" align="left"><font color="#000000">Context constraints and instruction tuning to limit model susceptibility.</font></p></li><li><p class="western" align="left"><font color="#000000">Use of guardrails and monitoring for anomalous prompt contents.</font></p></li><li><p class="western" align="left"><font color="#000000">Automated red teaming tools like <strong>Garak</strong> to detect prompt injection and jailbreak scenarios.</font></p></li></ul></li><li><p class="western" align="left"><font color="#000000">Ensuring strict prompt policies and runtime filtering aids secure deployment of LLMs.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1793391675FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Secure Model Training and Deployment Environments</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1534544193FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><font color="#000000">Training and deploying AI models in secure, isolated, and trusted environments reduce risk of compromise.</font></p></li><li><p class="western" align="left"><font color="#000000">Key safeguards:</font></p><ul><li><p class="western" align="left"><font color="#000000">Using <strong>Trusted Execution Environments (TEEs)</strong> or hardware-based secure enclaves.</font></p></li><li><p class="western" align="left"><font color="#000000">Containerization and sandboxing for resource isolation.</font></p></li><li><p class="western" align="left"><font color="#000000">Secure and auditable model versioning and provenance tracking.</font></p></li><li><p class="western" align="left"><font color="#000000">Network segmentation and zero trust controls limiting access to training and inference resources.</font></p></li></ul></li><li><p class="western" align="left"><font color="#000000">Continuous integration/continuous deployment (CI/CD) pipelines must include security checks and validations.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1487093953FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Model Monitoring and Drift Detection</div>
		<ul class="subexp">
	<li class="basic" id="FMID_482978251FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Model Drift</font></strong><font color="#000000"> occurs when the data distribution or environment changes, degrading model accuracy or safety.</font></p></li><li><p class="western" align="left"><font color="#000000">Ongoing monitoring includes:</font></p><ul><li><p class="western" align="left"><font color="#000000">Tracking input data statistics and feature distributions.</font></p></li><li><p class="western" align="left"><font color="#000000">Evaluating model predictions and confidence scores over time.</font></p></li><li><p class="western" align="left"><font color="#000000">Detecting concept drift or sudden anomalies.</font></p></li></ul></li><li><p class="western" align="left"><font color="#000000">Automated alerts and retraining triggers help maintain model reliability and security.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1803740849FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Zero Trust Architectures for AI</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1312366442FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><font color="#000000">Zero Trust is a security model based on “never trust, always verify.”</font></p></li><li><p class="western" align="left"><font color="#000000">Applied to AI, it enforces:</font></p><ul><li><p class="western" align="left"><font color="#000000">Strict identity and access controls for AI components.</font></p></li><li><p class="western" align="left"><font color="#000000">Continuous authentication and authorization for data, models, users, and services.</font></p></li><li><p class="western" align="left"><font color="#000000">Micro-segmentation of AI infrastructure.</font></p></li><li><p class="western" align="left"><font color="#000000">Policy enforcement at runtime for data usage and inference requests.</font></p></li></ul></li><li><p class="western" align="left"><font color="#000000">Zero trust reduces the attack surface by limiting lateral movement and enforcing least privilege.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_912658303FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Security Libraries and Toolkits</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1112729378FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">IBM Adversarial Robustness Toolbox (ART):</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Python library supporting many adversarial attacks and defenses.</font></p></li><li><p class="western" align="left"><font color="#000000">Compatible with frameworks like TensorFlow, PyTorch, scikit-learn.</font></p></li><li><p class="western" align="left"><font color="#000000">Enables robustness evaluation, mitigation strategies, and benchmarking.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_404821014FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">CleverHans:</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Open-source library for benchmarking adversarial attacks.</font></p></li><li><p class="western" align="left"><font color="#000000">Supports multiple attack algorithms and defensive techniques.</font></p></li><li><p class="western" align="left"><font color="#000000">Widely used in academic research and industry testing.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1041803521FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Foolbox:</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Focuses on creating adversarial examples and testing model robustness.</font></p></li><li><p class="western" align="left"><font color="#000000">Offers flexible APIs for integration with multiple ML frameworks.</font></p></li></ul></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1730859825FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Security Testing and Red Teaming Frameworks</div>
		<ul class="subexp">
	<li class="basic" id="FMID_854991845FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Microsoft Counterfit:</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Open-source AI security testing framework.</font></p></li><li><p class="western" align="left"><font color="#000000">Automates testing of AI models against adversarial attacks, data poisoning, model inversion.</font></p></li><li><p class="western" align="left"><font color="#000000">Integrates with CI/CD pipelines for continuous security evaluation.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_797496414FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Google SAIF (Security AI Framework):</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Framework designed to simulate attacks on AI systems.</font></p></li><li><p class="western" align="left"><font color="#000000">Enables proactive vulnerability discovery and defense testing.</font></p></li></ul></li></ul></div></li>
	<li class="basic" id="FMID_1700357189FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">AI Red Teaming Playbooks:</font></strong></p><ul><li><p class="western" align="left"><font color="#000000">Frameworks and methodologies for comprehensive adversarial testing.</font></p></li><li><p class="western" align="left"><font color="#000000">Includes threat modeling, scenario simulation, and mitigations.</font></p></li><li><p class="western" align="left"><font color="#000000">Emphasizes iterative testing and improvement.</font></p></li></ul></li></ul></div></li></ul></li>
	<li class="col" id="FMID_739081472FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Fuzzers and Penetration Testing Tools</div>
		<ul class="subexp">
	<li class="basic" id="FMID_729499081FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Fuzzing</font></strong><font color="#000000"> involves sending unexpected, malformed, or random inputs to AI models and associated software to uncover vulnerabilities.</font></p></li><li><p class="western" align="left"><font color="#000000">Tools like <strong>PyRIT</strong> (WiFi cracking) illustrate the use of AI in offensive security, while dedicated fuzzers target AI model APIs and preprocessing pipelines.</font></p></li><li><p class="western" align="left"><font color="#000000">Penetration testing of AI systems includes:</font></p><ul><li><p class="western" align="left"><font color="#000000">API security testing.</font></p></li><li><p class="western" align="left"><font color="#000000">Model inversion and extraction attempts.</font></p></li><li><p class="western" align="left"><font color="#000000">Exploiting misconfigurations in deployment.</font></p></li></ul></li><li><p class="western" align="left"><font color="#000000">Human-led manual pentesting complements automated tools to discover complex AI security issues.</font></p></li></ul></div></li></ul></li></ul></li>
	<li class="exp" id="FMID_238005974FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Regulatory, Ethical, and Responsible AI Practices</div>
		<ul class="sub">
	<li class="col" id="FMID_327220608FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Introduction</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1893616597FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western"><font color="#000000">As AI technologies become deeply embedded in critical decision-making and societal functions, <strong>regulatory compliance, ethics, and responsible AI practices</strong>emerge as vital pillars of AI security. Organizations must not only secure AI systems technically but also ensure their use respects legal standards, ethical norms, and societal values. This chapter provides an in-depth exploration of regulatory frameworks, ethical governance, fairness, accountability, transparency, and auditing in AI, equipping students with essential knowledge to navigate this complex landscape.</font></p><h2 class="western" style="line-height: 115%; margin-top: 0in; margin-bottom: 0.1in" /></div></li></ul></li>
	<li class="col" id="FMID_587212015FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Regulatory Compliance in AI</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1665978551FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><a name="__DdeLink__2515_1111379862" /><font color="#000000" size="3"><b>Overview of Key Regulations and Acts</b></font></p><p class="western"><font color="#000000">AI systems often process personal and sensitive data, making regulatory compliance mandatory to protect privacy, promote safety, and prevent misuse. Some of the most influential regulatory frameworks shaping AI security today include:</font></p><ul><li><p class="western"><strong><font color="#000000">General Data Protection Regulation (GDPR)</font></strong><font color="#000000">: An EU regulation focusing on data protection and privacy, GDPR sets stringent data processing requirements, including user consent, data minimization, and rights to explanation for automated decisions.</font></p></li><li><p class="western"><strong><font color="#000000">Health Insurance Portability and Accountability Act (HIPAA)</font></strong><font color="#000000">: In the US, HIPAA mandates protections for personal health information (PHI), affecting AI systems handling health data.</font></p></li><li><p class="western"><strong><font color="#000000">California Consumer Privacy Act (CCPA)</font></strong><font color="#000000">: US state-level privacy law granting consumers rights over their personal data collected by AI and other technologies.</font></p></li><li><p class="western"><strong><font color="#000000">EU AI Act (proposed)</font></strong><font color="#000000">: A pioneering legislative framework categorizing AI applications by risk with requirements for transparency, safety, human oversight, and accountability.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_1511636158FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Challenges in AI Regulatory Compliance</b></font></p><ul><li><p class="western"><strong><font color="#000000">Evolving and Fragmented Landscape:</font></strong><font color="#000000"> AI regulation is nascent and rapidly changing, with regional variations requiring organizations to adapt dynamically.</font></p></li><li><p class="western"><strong><font color="#000000">Data Privacy and Security:</font></strong><font color="#000000"> Ensuring secure data handling throughout AI lifecycles is critical to preventing breaches and unauthorized access.</font></p></li><li><p class="western"><strong><font color="#000000">Documentation and Traceability:</font></strong><font color="#000000"> Maintaining clear records of AI decision processes and interventions helps meet audit and enforcement standards.</font></p></li><li><p class="western"><strong><font color="#000000">Balancing Innovation and Compliance:</font></strong><font color="#000000"> Organizations must foster AI advancements while rigorously aligning with regulatory mandates to avoid penalties and reputational harm.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_626950272FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Best Practices for Compliance</b></font></p><ul><li><p class="western"><font color="#000000">Adopt frameworks like <strong>NIST AI Risk Management Framework (AI RMF)</strong>  that guide risk assessment, governance, and monitoring.</font></p></li><li><p class="western"><font color="#000000">Implement <strong>AI Bill of Materials (AI-BOM)</strong> to inventory models, data, and third-party tools for oversight.</font></p></li><li><p class="western"><font color="#000000">Embed privacy-by-design principles, such as data minimization and secure data pipelines.</font></p></li><li><p class="western"><font color="#000000">Conduct frequent internal audits and prepare for external regulatory inspections.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1393446699FM"><div class="nodecontent" style="color:#00b439;font-size:133%;"> Ethics, Governance, and Compliance</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1906321070FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Defining Ethics in AI</b></font></p><p class="western"><font color="#000000">Ethical AI involves designing and using AI systems that uphold human values, dignity, and rights. Core ethical principles include:</font></p><ul><li><p class="western"><strong><font color="#000000">Respect for Human Rights:</font></strong><font color="#000000"> Avoiding harm and discrimination.</font></p></li><li><p class="western"><strong><font color="#000000">Fairness:</font></strong><font color="#000000"> Ensuring equal treatment and preventing bias.</font></p></li><li><p class="western"><strong><font color="#000000">Transparency:</font></strong><font color="#000000"> Clear, understandable AI processes.</font></p></li><li><p class="western"><strong><font color="#000000">Accountability:</font></strong><font color="#000000"> Holding stakeholders responsible for AI impacts.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_764197860FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>AI Governance Structures</b></font></p><p class="western"><font color="#000000">Effective governance aligns ethical principles with organizational practices through:</font></p><ul><li><p class="western"><strong><font color="#000000">Policies and Codes of Conduct</font></strong><font color="#000000"> specifying acceptable AI use, data stewardship, and ethical boundaries.</font></p></li><li><p class="western"><strong><font color="#000000">Governance Bodies</font></strong><font color="#000000"> such as AI ethics boards or committees involving cross-functional members (legal, technical, business, and ethics).</font></p></li><li><p class="western"><strong><font color="#000000">Training and Awareness</font></strong><font color="#000000"> programs to sensitize developers, operators, and decision-makers to ethical concerns.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_359293871FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Compliance Beyond Legalities</b></font></p><p class="western"><font color="#000000">Ethical governance often extends past strict legal requirements to embody <strong>responsible AI use</strong>, balancing technical capabilities with societal expectations, and proactively managing emerging ethical dilemmas.</font></p></div></li></ul></li>
	<li class="col" id="FMID_751027036FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Fairness and Bias Mitigation</div>
		<ul class="subexp">
	<li class="basic" id="FMID_642644719FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>nderstanding Bias in AI</b></font></p><p class="western"><font color="#000000">Bias occurs when AI systems perpetuate or amplify unfair prejudices against individuals or groups, often due to:</font></p><ul><li><p class="western"><font color="#000000">Skewed or unrepresentative training data.</font></p></li><li><p class="western"><font color="#000000">Biased labeling or feature selection.</font></p></li><li><p class="western"><font color="#000000">Model overfitting to historical inequalities.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_528588571FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Mitigation Strategies</b></font></p><ul><li><p class="western"><strong><font color="#000000">Diverse Data Collection:</font></strong><font color="#000000"> Ensuring datasets cover all relevant demographics and contexts.</font></p></li><li><p class="western"><strong><font color="#000000">Preprocessing Techniques:</font></strong><font color="#000000"> Data sanitization, rebalancing, and anonymization.</font></p></li><li><p class="western"><strong><font color="#000000">In-processing Methods:</font></strong><font color="#000000"> Regularization, adversarial debiasing during training.</font></p></li><li><p class="western"><strong><font color="#000000">Post-processing Adjustments:</font></strong><font color="#000000"> Calibrating outputs to reduce disparate impacts.</font></p></li><li><p class="western"><strong><font color="#000000">Continuous Monitoring:</font></strong><font color="#000000"> Detecting bias drift and unintended consequences during deployment.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_748304818FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Accountability, Explainability, and Transparency</div>
		<ul class="subexp">
	<li class="basic" id="FMID_867822203FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Accountability Mechanisms</b></font></p><p class="western"><font color="#000000">Organizations must establish clear accountability for AI outcomes, including:</font></p><ul><li><p class="western"><font color="#000000">Defining responsible parties for design, deployment, and oversight.</font></p></li><li><p class="western"><font color="#000000">Instituting audit trails documenting decision rationale.</font></p></li><li><p class="western"><font color="#000000">Enabling mechanisms for redress and correction.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_1025232240FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Explainability</b></font></p><p class="western"><font color="#000000">Explainable AI (XAI) aims to make AI decisions understandable to humans, helping:</font></p><ul><li><p class="western"><font color="#000000">Build user trust.</font></p></li><li><p class="western"><font color="#000000">Facilitate compliance with legal rights to explanation.</font></p></li><li><p class="western"><font color="#000000">Support debugging and improvement by developers.</font></p></li></ul><p class="western"><font color="#000000">Techniques include model-agnostic methods (LIME, SHAP), interpretable models (decision trees), and visualizations.</font></p></div></li>
	<li class="basic" id="FMID_733321543FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Transparency Practices</b></font></p><ul><li><p class="western"><font color="#000000">Disclose AI system capabilities, limitations, and data usage to users.</font></p></li><li><p class="western"><font color="#000000">Provide clear notices when users interact with automated decision systems.</font></p></li><li><p class="western"><font color="#000000">Publish regular reports on AI performance, risks, and governance.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_710179957FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Ethical Use of AI</div>
		<ul class="subexp">
	<li class="basic" id="FMID_500908082FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western"><font color="#000000">Ethical AI use requires organizations to:</font></p><ul><li><p class="western"><font color="#000000">Avoid deploying systems that cause harm or violate human rights.</font></p></li><li><p class="western"><font color="#000000">Ensure AI complements, rather than replaces, requisite human judgment.</font></p></li><li><p class="western"><font color="#000000">Respect user autonomy with opt-in/opt-out choices.</font></p></li><li><p class="western"><font color="#000000">Consider societal impact, including environmental and labor effects.</font></p></li><li><p class="western"><font color="#000000">Engage stakeholders in ongoing dialogue about AI ethics and governance.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1106399848FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Auditing and Responsible AI Principles</div>
		<ul class="subexp">
	<li class="basic" id="FMID_441398742FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>AI Auditing</b></font></p><p class="western"><font color="#000000">AI audits are independent evaluations verifying that:</font></p><ul><li><p class="western"><font color="#000000">AI systems operate within defined ethical, legal, and security frameworks.</font></p></li><li><p class="western"><font color="#000000">Models meet performance, fairness, and robustness requirements.</font></p></li><li><p class="western"><font color="#000000">Data handling complies with privacy and security regulations.</font></p></li></ul><p class="western"><font color="#000000">Audits utilize:</font></p><ul><li><p class="western"><font color="#000000">Automated tools for bias detection, drift analysis, and vulnerability scans.</font></p></li><li><p class="western"><font color="#000000">Manual code reviews and stakeholder interviews.</font></p></li><li><p class="western"><font color="#000000">Documentation verification and impact assessments.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_169609336FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Responsible AI Principles</b></font></p><p class="western"><font color="#000000">Leading organizations and consortia advocate responsible AI principles, including:</font></p><ul><li><p class="western"><strong><font color="#000000">Safety:</font></strong><font color="#000000"> Minimize unintended harms.</font></p></li><li><p class="western"><strong><font color="#000000">Privacy:</font></strong><font color="#000000"> Protect personal data rigorously.</font></p></li><li><p class="western"><strong><font color="#000000">Fairness:</font></strong><font color="#000000"> Prevent discrimination.</font></p></li><li><p class="western"><strong><font color="#000000">Transparency:</font></strong><font color="#000000"> Clear communication.</font></p></li><li><p class="western"><strong><font color="#000000">Human Oversight:</font></strong><font color="#000000"> Maintain human-in-the-loop controls.</font></p></li><li><p class="western"><strong><font color="#000000">Sustainability:</font></strong><font color="#000000"> Consider broader societal effects.</font></p></li></ul><p class="western"><font color="#000000">Adoption of these principles supports long-term trust, legitimacy, and regulatory acceptance.</font></p></div></li></ul></li></ul></li>
	<li class="exp" id="FMID_1812010789FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Risk and Incident Management in AI Security</div>
		<ul class="sub">
	<li class="col" id="FMID_1815585869FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Introduction</div>
		<ul class="subexp">
	<li class="basic" id="FMID_745115346FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western"><font color="#000000">In the evolving landscape of Artificial Intelligence, <strong>Risk and Incident Management</strong> is critical to safeguarding AI systems against threats that can compromise model integrity, privacy, safety, and organizational reputation. AI introduces unique risks—from data poisoning to adversarial manipulation—that require specialized risk management and incident response strategies. This chapter explores foundational concepts, highlights AI-specific requirements, and presents MAESTRO, a state-of-the-art AI threat modeling framework designed for complex AI deployments.</font></p></div></li></ul></li>
	<li class="col" id="FMID_1864700147FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Foundations of Risk and Incident Management in AI</div>
		<ul class="subexp">
	<li class="col" id="FMID_312554708FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western"><strong><font color="#000000">Risk Management</font></strong><font color="#000000">in AI involves systematically identifying, assessing, and mitigating vulnerabilities and threats associated with AI technologies throughout their lifecycle.</font></p><p class="western"><strong><font color="#000000">Incident Management</font></strong><font color="#000000">is the structured approach to preparing for, detecting, analyzing, and responding to security incidents affecting AI systems to minimize damage and restore normal operations.</font></p></div>
		<ul class="subexp">
	<li class="basic" id="FMID_1971177983FM"><div class="nodecontent" style="color:#111111;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Objectives</b></font></p><ul><li><p class="western"><strong><font color="#000000">Proactive Risk Identification:</font></strong><font color="#000000"> Detect potential vulnerabilities in datasets, algorithms, deployment environments, and operational settings.</font></p></li><li><p class="western"><strong><font color="#000000">Continuous Monitoring:</font></strong><font color="#000000"> Track AI system behavior and performance to identify deviations or attacks in real-time.</font></p></li><li><p class="western"><strong><font color="#000000">Timely Incident Response:</font></strong><font color="#000000"> Establish processes to respond quickly and effectively to AI-related security incidents.</font></p></li></ul></div></li></ul></li></ul></li>
	<li class="col" id="FMID_1088641479FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">AI-Focused Risk Management Practices</div>
		<ul class="subexp">
	<li class="col" id="FMID_701488000FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western"><font color="#000000">AI systems present distinct risk vectors that traditional IT risk management frameworks only partially address. Effective AI risk management integrates these specialized considerations.</font></p></div>
		<ul class="subexp">
	<li class="basic" id="FMID_150684049FM"><div class="nodecontent" style="color:#111111;"><p class="western" style="line-height: 115%"><b><font color="#000000" size="3">Key AI Risk Areas</font></b></p><ul><li><p class="western"><strong><font color="#000000">Data Integrity Risks:</font></strong><font color="#000000"><br />Includes data poisoning attacks where malicious data corrupts training processes, leading to faulty or biased models.</font></p></li><li><p class="western"><strong><font color="#000000">Model Vulnerabilities:</font></strong><font color="#000000"><br />Models are susceptible to adversarial examples crafted to mislead AI outputs or model extraction that leaks proprietary knowledge.</font></p></li><li><p class="western"><strong><font color="#000000">Supply Chain Risks:</font></strong><font color="#000000"><br />Third-party datasets, pre-trained models, and libraries may introduce unknown vulnerabilities or compliance issues.</font></p></li><li><p class="western"><strong><font color="#000000">Deployment and Runtime Risks:</font></strong><font color="#000000"><br />Expose models to API abuse, inference attacks, or unauthorized access, potentially corrupting outcomes or leaking sensitive data.</font></p></li><li><p class="western"><strong><font color="#000000">Ethical and Compliance Risks:</font></strong><font color="#000000"><br />Models may inadvertently produce biased or unfair outcomes risking regulatory sanctions and reputational damage.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_1760428086FM"><div class="nodecontent" style="color:#111111;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>AI Risk Management Frameworks and Principles</b></font></p><p class="western"><font color="#000000">The <strong>NIST AI Risk Management Framework (AI RMF)</strong>is a leading guideline focusing on four core functions:</font></p><table width="496" cellpadding="2" cellspacing="0"><col width="65" /><col width="423" /><tr><th width="65" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Function
          </p></th><th width="423" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Description
          </p></th></tr><tr><td width="65" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Govern</strong></p></td><td width="423" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Establish accountability, policies, and risk tolerance
          </p></td></tr><tr><td width="65" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Map</strong></p></td><td width="423" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Identify and categorize AI-related risks by context and use case
          </p></td></tr><tr><td width="65" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Measure</strong></p></td><td width="423" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Quantitatively and qualitatively assess risk impact and likelihood
          </p></td></tr><tr><td width="65" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Manage</strong></p></td><td width="423" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Prioritize and implement risk mitigation strategies
          </p></td></tr></table></div></li>
	<li class="basic" id="FMID_438409548FM"><div class="nodecontent" style="color:#111111;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Best Practices</b></font></p><ul><li><p class="western"><font color="#000000">Implement <strong>zero-trust access</strong> and role-based permissions around AI training data, models, and APIs.</font></p></li><li><p class="western"><font color="#000000">Use <strong>adversarial training and testing</strong> to improve model resilience.</font></p></li><li><p class="western"><font color="#000000">Deploy <strong>continuous monitoring systems</strong> that detect model drift, anomalous outputs, or misuse with alerts.</font></p></li><li><p class="western"><font color="#000000">Conduct <strong>regular risk assessments and audits</strong> focused on AI-specific threats.</font></p></li><li><p class="western"><font color="#000000">Maintain an <strong>AI Bill of Materials (AI-BOM)</strong> to document dependencies and supply chain components for transparency.</font></p></li></ul></div></li></ul></li></ul></li>
	<li class="col" id="FMID_776491990FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Incident Response Planning for AI Systems</div>
		<ul class="subexp">
	<li class="col" id="FMID_480443932FM"><div class="nodecontent" style="color:#990000;font-size:117%;">A well-defined incident response (IR) plan tailored for AI incidents is essential to minimize operational disruptions and mitigate harm.</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1463937050FM"><div class="nodecontent" style="color:#111111;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Unique Characteristics of AI Incidents</b></font></p><ul><li><p class="western"><font color="#000000">AI incidents may involve <strong>model manipulation</strong>, <strong>data tampering</strong>, <strong>exploitation of inference APIs</strong>, or <strong>privacy breaches</strong>  through model inversion.</font></p></li><li><p class="western"><font color="#000000">Detection requires monitoring both technical security metrics and model performance anomalies.</font></p></li><li><p class="western"><font color="#000000">Response often necessitates <strong>model retraining</strong>, <strong>data forensic analysis</strong>, and sometimes <strong>ethical review</strong> for unintended biases.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_1771543856FM"><div class="nodecontent" style="color:#111111;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Incident Response Lifecycle for AI</b></font></p><table width="643" cellpadding="2" cellspacing="0"><col width="193" /><col width="442" /><tr><th width="193" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Phase
          </p></th><th width="442" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Key Activities
          </p></th></tr><tr><td width="193" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Preparation</strong></p></td><td width="442" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Define team roles, develop AI-specific IR playbooks, train employees, and deploy monitoring tools.
          </p></td></tr><tr><td width="193" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Detection &amp; Analysis</strong></p></td><td width="442" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Use automated alerts, anomaly detection, and forensic investigation to identify the incident nature and scope.
          </p></td></tr><tr><td width="193" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Containment, Eradication &amp; Recovery</strong></p></td><td width="442" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Isolate affected models/systems, remove malicious artifacts, retrain or rollback models, restore service securely.
          </p></td></tr><tr><td width="193" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western"><strong>Post-Incident Activity</strong></p></td><td width="442" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Review incident lessons, update security controls and governance, report to stakeholders or regulators.
          </p></td></tr></table></div></li>
	<li class="basic" id="FMID_1078712751FM"><div class="nodecontent" style="color:#111111;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Integrating AI Governance into Incident Response</b></font></p><ul><li><p class="western"><font color="#000000">Include <strong>cross-functional teams</strong> encompassing AI developers, security analysts, legal/compliance officers, and ethics advisors.</font></p></li><li><p class="western"><font color="#000000">Leverage incident data to inform <strong>future risk assessments</strong> and governance adjustments.</font></p></li><li><p class="western"><font color="#000000">Ensure documentation supports regulatory requirements such as GDPR breach notifications.</font></p></li></ul></div></li></ul></li></ul></li>
	<li class="col" id="FMID_336197852FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">AI-Specific Threat Modeling and the MAESTRO Framework</div>
		<ul class="subexp">
	<li class="col" id="FMID_565169394FM"><div class="nodecontent" style="color:#990000;font-size:117%;">Traditional threat modeling frameworks like STRIDE or PASTA cover general IT risks but lack focus on AI’s autonomous, data-dependent, and multi-agent properties.</div>
		<ul class="subexp">
	<li class="basic" id="FMID_420911939FM"><div class="nodecontent" style="color:#111111;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Overview of MAESTRO</b></font></p><p class="western"><strong><font color="#000000">MAESTRO</font></strong><font color="#000000">(Multi-Agent Environment Security Threat &amp; Risk Ontology) is an AI-focused threat modeling framework designed specifically to address the complexities of agentic and autonomous AI systems. It:</font></p><ul><li><p class="western"><font color="#000000">Models diverse AI agents, their goals, communication, and interactions within ecosystems.</font></p></li><li><p class="western"><font color="#000000">Breaks down AI architecture into layered components (e.g., perception, learning, communication).</font></p></li><li><p class="western"><font color="#000000">Identifies AI-specific risks such as prompt injection, model drift, inter-agent deception, and autonomous decision errors.</font></p></li><li><p class="western"><font color="#000000">Supports continuous risk evaluation and dynamic adaptation to evolving AI behaviors.</font></p></li></ul></div></li>
	<li class="basic" id="FMID_864733899FM"><div class="nodecontent" style="color:#111111;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Seven-Layer Structure of MAESTRO</b></font></p><table width="642" cellpadding="2" cellspacing="0"><col width="197" /><col width="437" /><tr><th width="197" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Layer
          </p></th><th width="437" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Description
          </p></th></tr><tr><td width="197" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            1. Foundational Models
          </p></td><td width="437" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Core AI models and algorithms foundational to the system
          </p></td></tr><tr><td width="197" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            2. Data Operations
          </p></td><td width="437" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Data sourcing, preprocessing, storage, and pipeline security
          </p></td></tr><tr><td width="197" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            3. Agent Frameworks
          </p></td><td width="437" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Individual agent decision-making, learning, and reasoning modules
          </p></td></tr><tr><td width="197" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            4. Deployment Infrastructure
          </p></td><td width="437" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Hosting environments, APIs, and runtime controls
          </p></td></tr><tr><td width="197" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            5. Evaluation &amp; Observability
          </p></td><td width="437" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Monitoring, logging, and performance assessment
          </p></td></tr><tr><td width="197" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            6. Security &amp; Compliance
          </p></td><td width="437" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Controls for privacy, access, legal, and regulatory adherence
          </p></td></tr><tr><td width="197" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            7. Agent Ecosystem
          </p></td><td width="437" style="border-top-style: none; border-top-width: medium; border-right-style: none; border-right-width: medium; border-bottom-style: none; border-bottom-width: medium; border-left-style: none; border-left-width: medium; padding-top: 0in; padding-right: 0in; padding-bottom: 0in; padding-left: 0in"><p class="western">
            Inter-agent communication, collaboration, and network effects
          </p></td></tr></table></div></li>
	<li class="basic" id="FMID_803019964FM"><div class="nodecontent" style="color:#111111;"><p class="western" style="line-height: 115%"><font color="#000000" size="3"><b>Advantages of MAESTRO</b></font></p><ul><li><p class="western"><font color="#000000">Holistic multidimensional analysis tailored for AI’s autonomous and emergent behavior.</font></p></li><li><p class="western"><font color="#000000">Enables tracing of security risks to specific layers and agent interactions.</font></p></li><li><p class="western"><font color="#000000">Supports layered defense strategies that address technical, operational, and ethical vulnerabilities.</font></p></li><li><p class="western"><font color="#000000">Facilitates continuous monitoring and refinement with real-time feedback loops.</font></p></li></ul></div></li></ul></li></ul></li>
	<li class="col" id="FMID_1511019252FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Practical Recommendations and Emerging Trends</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1535232702FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western"><font color="#000000">Establish <strong>AI-centric incident red teams</strong> to simulate adversarial attacks and test response readiness.</font></p></li><li><p class="western"><font color="#000000">Integrate <strong>AI risk management platforms</strong> that automate compliance, bias detection, and runtime protection.</font></p></li><li><p class="western"><font color="#000000">Adopt <strong>AI-specific penetration testing</strong> focusing on prompt manipulation, adversarial examples, and model inversion.</font></p></li><li><p class="western"><font color="#000000">Continuously update incident response plans to include novel attack vectors from evolving AI capabilities such as generative models and autonomous agents.</font></p></li><li><p class="western"><font color="#000000">Participate in <strong>community threat intelligence sharing</strong> around AI threats to stay ahead of emerging risks.</font></p></li></ul></div></li></ul></li></ul></li>
	<li class="exp" id="FMID_1770307474FM"><div class="nodecontent" style="color:#0033ff;font-size:150%;">Emerging Trends and Research Directions in AI Security</div>
		<ul class="sub">
	<li class="col" id="FMID_1176142966FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Explainable AI Security (XAI)</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1575539336FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Explainable AI Security focuses on making AI decisions understandable and interpretable by humans to enhance trust, accountability, and debugging capabilities. With growing regulatory demands (e.g., GDPR’s right to explanation), XAI techniques help reveal how inputs influence AI outputs, especially when securing AI against adversarial manipulations.</font></p><ul><li><p class="western" align="left"><font color="#000000">Methods include saliency maps, model distillation, counterfactual explanations, and transparent architectures.</font></p></li><li><p class="western" align="left"><font color="#000000">Explainability supports detecting anomalies, auditing decisions, and improving defensive strategies by exposing model behavior in security contexts.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_289087208FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Privacy-Preserving Machine Learning</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1898096214FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Privacy preservation is crucial in protecting sensitive data during AI training and inference. Key approaches include:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Federated Learning:</font></strong><font color="#000000"> Multiple parties collaboratively train AI models without sharing raw data, reducing privacy risks.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Secure Multi-Party Computation (MPC):</font></strong><font color="#000000"> Allows computation on encrypted data so no party learns other parties’ inputs.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Differential Privacy:</font></strong><font color="#000000"> Introduces noise to training data or queries to protect individual data points from identification.</font></p></li><li><p class="western" align="left"><font color="#000000">Advances here enable AI use in sensitive domains like healthcare, finance, and government while maintaining compliance with GDPR, HIPAA, and more.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_342638710FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Formal Verification for AI Security</div>
		<ul class="subexp">
	<li class="basic" id="FMID_833069364FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Formal verification applies mathematical techniques to prove AI model properties such as robustness, safety, and correctness in the face of adversarial inputs and environmental changes.</font></p><ul><li><p class="western" align="left"><font color="#000000">Verification tools rigorously check for vulnerabilities like adversarial example resistance, ensuring constraints hold under all possible inputs.</font></p></li><li><p class="western" align="left"><font color="#000000">This field is emerging for neural networks and reinforcement learning models, using techniques like SMT solvers, abstract interpretation, and model checking.</font></p></li><li><p class="western" align="left"><font color="#000000">Formal guarantees increase confidence in safety-critical AI applications (e.g., autonomous vehicles, medical diagnostics).</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_244177171FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Defense Against Zero-Day and Unknown Threats</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1591333494FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Traditional signature-based defenses are ineffective against zero-day AI threats (unknown, novel attacks).</font></p><ul><li><p class="western" align="left"><font color="#000000">Emerging strategies include anomaly detection using AI monitoring, behavior-based defenses, and honeytokens for early warning.</font></p></li><li><p class="western" align="left"><font color="#000000">Red teaming and adversarial testing simulate novel attack techniques to evaluate and harden AI models pre-deployment.</font></p></li><li><p class="western" align="left"><font color="#000000">Continuous learning and adaptive security systems help AI defend itself by evolving with the threat landscape.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_681474106FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Secure AI Hardware</div>
		<ul class="subexp">
	<li class="basic" id="FMID_1688018003FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Hardware designed specifically for AI — including AI accelerators and neuromorphic chips — introduces new security considerations and opportunities:</font></p><ul><li><p class="western" align="left"><font color="#000000">AI-specific chips optimize performance but must integrate hardware security features like trusted execution environments (TEE), secure boot, and encryption.</font></p></li><li><p class="western" align="left"><font color="#000000">Hardware root of trust helps protect models and data from tampering and side-channel attacks.</font></p></li><li><p class="western" align="left"><font color="#000000">Research is advancing on protection against hardware Trojans and supply chain vulnerabilities specific to AI hardware.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_1919905893FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Quantum-Resistant Algorithms for AI</div>
		<ul class="subexp">
	<li class="basic" id="FMID_429341431FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">With the advent of quantum computing potentially breaking classical cryptography, AI security is preparing:</font></p><ul><li><p class="western" align="left"><font color="#000000">Quantum-resistant (post-quantum) cryptographic algorithms secure communications, data, and model integrity against quantum attacks.</font></p></li><li><p class="western" align="left"><font color="#000000">Integration of post-quantum algorithms in AI systems preserves long-term confidentiality, especially for sensitive AI workloads.</font></p></li><li><p class="western" align="left"><font color="#000000">Research also investigates quantum-safe protocols for federated learning and secure MP</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_15202042FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Advanced Adversarial Research</div>
		<ul class="subexp">
	<li class="basic" id="FMID_671282982FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><p class="western" align="left"><font color="#000000">Research continues to evolve on adversarial AI:</font></p><ul><li><p class="western" align="left"><strong><font color="#000000">Red Teaming:</font></strong><font color="#000000"> Simulated adversarial attacks evaluate AI defenses under realistic conditions, exposing weak points.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Evaluation Metrics:</font></strong><font color="#000000"> Improved benchmarks and challenge datasets provide rigorous testing for robustness and fairness.</font></p></li><li><p class="western" align="left"><font color="#000000">New attack vectors, including prompt injections in LLMs and model extraction, drive constant innovation in defense techniques.</font></p></li></ul></div></li></ul></li>
	<li class="col" id="FMID_617911298FM"><div class="nodecontent" style="color:#00b439;font-size:133%;">Post-Quantum and Neurosymbolic AI Security</div>
		<ul class="subexp">
	<li class="basic" id="FMID_127085560FM"><div class="nodecontent" style="color:#990000;font-size:117%;"><ul><li><p class="western" align="left"><strong><font color="#000000">Post-Quantum AI Security</font></strong><font color="#000000"> focuses on safeguarding AI against threats posed by quantum computing capabilities.</font></p></li><li><p class="western" align="left"><strong><font color="#000000">Neurosymbolic AI</font></strong><font color="#000000"> combines neural networks with symbolic reasoning for more interpretable, robust, and secure AI.</font></p></li><li><p class="western" align="left"><font color="#000000">Integration of symbolic logic enables formal reasoning about AI decisions, aiding verification and explainability.</font></p></li><li><p class="western" align="left"><font color="#000000">These hybrid models may enhance AI security by making models less prone to adversarial manipulation and better aligned with human-understandable logic.</font></p></li></ul></div></li></ul></li></ul></li></ul></li></ul></div></body></html>
